{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #defining neural network\n",
    "import torch.nn.functional as F #helper\n",
    "from torch.utils.data import DataLoader #batching\n",
    "from torchvision import datasets, transforms #loading datasets\n",
    "from torchvision.utils import make_grid  #for visualization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1baada20850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm tranform (0,255 -> 0,1)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load data\n",
    "train_data = datasets.MNIST(root=\"datasets\",train=True,download=True,transform=transform)\n",
    "\n",
    "# validation set\n",
    "train_set,val_set = torch.utils.data.random_split(train_data,[50000,10000])\n",
    "\n",
    "#test\n",
    "test_set  = datasets.MNIST(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: datasets\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size = 64,shuffle=True)\n",
    "val_loader = DataLoader(val_set,batch_size = 64,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size = 10000,shuffle=False) # no need to shuffle and batch cuda will not out of memory because no grad cal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) # sample 782*64 = 50048 (50000 sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for images,labels in train_loader:\n",
    "    break\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(np.transpose(images[0],(1,2,0)))\n",
    "print(\"label: \",labels[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$O = \\frac{I-F+2P}{S} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv1d = time series, language, signal\n",
    "# conv2d = images, spectrum\n",
    "# conv3d = video\n",
    "# [1,3,]\n",
    "# nn.ModuleList()\n",
    "\n",
    "layer1 = nn.Conv2d(in_channels=images.shape[1],out_channels=3,kernel_size=(5,3),stride=1,padding=(2,1))\n",
    "output = layer1(images)\n",
    "output.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = layer1(images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150528])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 0\n",
    "class POOH_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self,input_shape,output_shape,conv_lc = [],linear_lc =[]):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        everything square because it easy\n",
    "\n",
    "        input_shape: int \n",
    "        output_shape: int\n",
    "        linear_lc : [node1,node2,...]\n",
    "        conv_lc : [input,filter, ..]   input => (chanel)    filter => (chanel,kernal size) \n",
    "        \"\"\"\n",
    "\n",
    "        self.conv_list = nn.ModuleList([\n",
    "                                        nn.Conv2d(conv_lc[i][0],conv_lc[i+1][0],kernel_size=conv_lc[i+1][1] ,stride=1,padding=1) \n",
    "                                        for i in range(len(conv_lc[:-1])) \n",
    "                                        ])\n",
    "        \n",
    "        curr_shape = input_shape\n",
    "        for c in conv_lc[1:]:\n",
    "            curr_shape = self.cal_convshape(I = curr_shape,F=c[1])\n",
    "\n",
    "        self.conv_out = int(conv_lc[-1][0]*curr_shape**2)\n",
    "\n",
    "        linear_lc = [self.conv_out]+linear_lc+[output_shape]\n",
    "\n",
    "        self.linear_list = nn.ModuleList([\n",
    "                                        nn.Linear(linear_lc[i],linear_lc[i+1])\n",
    "                                          for i in range(len(linear_lc[:-1]))\n",
    "                                          ])\n",
    "\n",
    "    def cal_convshape(self,I,F,P=1,S=1):\n",
    "        return ((I-F+2*P)/S)+1\n",
    "    \n",
    "    def cal_pad(self,F,S):\n",
    "        pass\n",
    "\n",
    "    def forward(self,images):\n",
    "        \n",
    "        output = images\n",
    "        for layer in self.conv_list:\n",
    "            output = F.relu(layer(output))\n",
    "\n",
    "        output = output.reshape(-1,self.conv_out)\n",
    "        for layer in self.linear_list[:-1]:\n",
    "            output = F.relu(layer(output))\n",
    "\n",
    "        output = self.linear_list[-1](output) \n",
    "        \n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test first\n",
    "\n",
    "model = POOH_CNN(input_shape = images.shape[-1],output_shape=10,conv_lc=[(images.shape[1],None),(5,3),(10,3)],linear_lc=[120,84])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=7840, out_features=120, bias=True)\n",
       "  (1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (2): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  = model(images)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    45\n",
      "     5\n",
      "   450\n",
      "    10\n",
      "940800\n",
      "   120\n",
      " 10080\n",
      "    84\n",
      "   840\n",
      "    10\n",
      "______\n",
      "952444\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        y_hat = model(images)\n",
    "        loss  = criterion(y_hat, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028779927641153336"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entrophy Loss:  0.08101659268140793\n"
     ]
    }
   ],
   "source": [
    "model.eval() #this will turn off dropout, batch norm\n",
    "with torch.no_grad():  #this will turn off gradient calculations\n",
    "    for images, labels in test_loader:\n",
    "        y_hat = model(images)\n",
    "        loss = criterion(y_hat, labels)\n",
    "        \n",
    "print(\"Cross Entrophy Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_pred = torch.max(y_hat, 1)[1]\n",
    "y_hat_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 974,    0,    3,    2,    1,    4,    7,    0,    3,    4],\n",
       "       [   1, 1127,    1,    0,    1,    0,    4,    5,    1,    6],\n",
       "       [   0,    2, 1021,   14,    1,    0,    0,   15,    7,    1],\n",
       "       [   0,    1,    2,  973,    0,    6,    1,    0,    3,    1],\n",
       "       [   0,    1,    1,    0,  970,    0,    6,    1,    2,   18],\n",
       "       [   1,    0,    0,    6,    0,  858,    5,    0,    0,    4],\n",
       "       [   3,    2,    0,    0,    5,    4,  921,    0,    2,    0],\n",
       "       [   1,    1,    3,    7,    0,    2,    0, 1004,    2,   14],\n",
       "       [   0,    1,    1,    5,    3,   15,   14,    1,  951,   11],\n",
       "       [   0,    0,    0,    3,    1,    3,    0,    2,    3,  950]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#confusion matrix\n",
    "confusion_matrix(y_hat_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_set[0][0].shape\n",
    "test_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.2589,  -2.3991,   1.3148,  -0.0519, -10.6724,  -5.8069, -17.0051,\n",
       "          12.4333,  -5.4502,  -1.7179]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pred = model(test_set[0][0])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
